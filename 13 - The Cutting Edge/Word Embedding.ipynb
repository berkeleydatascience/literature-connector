{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "This lesson is designed to explore features of word embeddings described by Ben Schmidt in his blog post <a href=\"http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html\">\"Rejecting the Gender Binary\"</a>.\n",
    "\n",
    "The primary corpus we use consists of the <a href=\"http://txtlab.org/?p=601\">150 English-language novels</a> made available by the .txtLab at McGill. We also look at a <a href=\"http://ryanheuser.org/word-vectors-1/\">Word2Vec model trained on the ECCO-TCP corpus</a> of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (I have shortened the number of terms in the model by half in order to conserve memory.)\n",
    "\n",
    "For background on Word2Vec's mechanics, I suggest this <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html\">brief tutorial</a> by Google, especially the sections \"Motivation,\" \"Skip-Gram Model,\" and \"Visualizing.\"\n",
    "\n",
    "### Lesson\n",
    "<ol>\n",
    "<li>Pre-Processing</li>\n",
    "<li>Word2Vec</li>\n",
    "<ol><li>Training</li>\n",
    "<li>Embeddings</li>\n",
    "<li>Visualization</li>\n",
    "</ol>\n",
    "<li>Saving/Loading Models</li>\n",
    "<li>Word2Vec from Scratch</li>\n",
    "<li>Appendix: Vector Rejection</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Data Wrangling\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Note: 'datascience' is a custom package used by Berkeley's\n",
    "# Foundations of Data Science course. In practice it is very\n",
    "# similar to the more-standard 'pandas'\n",
    "\n",
    "# Natural Language Processing\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Description\n",
    "English-language subset of Andrew Piper's novel corpus, totaling 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n",
    "\n",
    "### Metadata Columns\n",
    "<ol><li>Filename: Name of file on disk</li>\n",
    "<li>ID: Unique ID in Piper corpus</li>\n",
    "<li>Language: Language of novel</li>\n",
    "<li>Date: Initial publication date</li>\n",
    "<li>Title: Title of novel</li>\n",
    "<li>Gender: Authorial gender</li>\n",
    "<li>Person: Textual perspective</li>\n",
    "<li>Length: Number of tokens in novel</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Metadata\n",
    "metadata_tb = Table.read_table('txtlab_Novel150_English.csv')\n",
    "\n",
    "\n",
    "# Alternately, using pandas:\n",
    "\n",
    "# import pandas\n",
    "# metadata_tb = pandas.read_csv('txtlab_Novel150_English.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set location of corpus folder\n",
    "fiction_path = 'txtalb_Novel150_English/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty list, entries will be list of tokens from each novel\n",
    "novel_list = []\n",
    "\n",
    "# Iterate through filenames in metadata table\n",
    "for filename in metadata_tb['filename']:\n",
    "    \n",
    "    # Read in novel text as single string, make lowercase\n",
    "    with open(fiction_path+filename, 'r') as file_in:\n",
    "        novel = file_in.read()\n",
    "    \n",
    "    # Add novel text as single string to master list\n",
    "    novel_list.append(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec learns about the relationships among words by observing them in context. We'll need to tokenize the words in our corpus while retaining sentence boundaries. Since novels were imported as single strings, we'll first use <i>sent_tokenize</i> to divide them into sentences, and second, we'll split each sentence into its own list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Even though I love nltk's word_tokenize(), I just need a\n",
    "# quick and dirty tokenizer. Emphasis on quick.\n",
    "\n",
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    from string import punctuation\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in text if char not in punctuation])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_by_sentence = [fast_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since fast_tokenize() is a little clunky, we'll explicitly remove\n",
    "# any sentences that contain zero tokens\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "words_by_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Embedding\n",
    "Word2Vec is the most prominent word embedding algorithm. Word embedding generally attempts to identify semantic relationships between words by observing them in context.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of Word2Vec are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. Skip-Gram takes a word of interest as its input (e.g. \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\"). CBOW does the opposite, taking the context words (\"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Size: Number of dimensions for word embedding model</li>\n",
    "<li>Window: Number of context words to observe in each direction</li>\n",
    "<li>min_count: Minimum frequency for words included in model</li>\n",
    "<li>sg (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram</li>\n",
    "<li>Alpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Iterations: Number of passes through dataset</li>\n",
    "<li>Batch Size: Number of words to sample from data during each pass</li>\n",
    "</ul>\n",
    "\n",
    "Note: Script uses default value for each argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train word2vec using CBOW\n",
    "model = gensim.models.Word2Vec(words_by_sentence, size=100, window=5, \\\n",
    "                               min_count=5, sg=0, alpha=0.025, iter=5, batch_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return dense word vector\n",
    "model['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cosine distance between two word vectors\n",
    "model.similarity('sense','sensibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cosine distance between two clusters of word vectors\n",
    "# Each cluster is measured as the mean of its words\n",
    "\n",
    "model.n_similarity(['sense','sensibility'],['whale','harpoon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finds mean vector of words in list\n",
    "# and identifies the word further from that mean\n",
    "\n",
    "model.doesnt_match(['pride','prejudice', 'harpoon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The canonic word2vec exercise: King - Man + Woman -> Queen\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can we find gender a la Schmidt?\n",
    "# Note that this method uses projection, whereas Schmidt had used rejection\n",
    "\n",
    "model.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['she','her','hers','herself','he','him','his','himself'], topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Use the most_similar method to find the tokens nearest to 'car' in our model.\n",
    "##     Do the same for 'motorcar'.\n",
    "\n",
    "## Q.  What characterizes each word in our corpus? Does this make sense?\n",
    "\n",
    "\n",
    "## EX. Vector addition and subtraction can be thought of in terms of analogy.\n",
    "##     From the example above: 'man' is to 'king' as 'woman' is to '???'\n",
    "##     Use the most_similar method to find: 'paris' is to 'france' as 'london' is to '???'\n",
    "\n",
    "## Q.  What has our model learned about nation-states?\n",
    "\n",
    "\n",
    "## EX. Perform the canonic Word2Vec addition again but leave out a term:\n",
    "##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n",
    "\n",
    "## Q.  What do these indicate semantically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of words in model\n",
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the whole vocabulary would make it hard to read\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For interpretability, we'll select words that already have a semantic relation\n",
    "\n",
    "her_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n",
    "                                                       negative=['he','him','his','himself'], topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the vector for each sampled word\n",
    "vectors = [model[word] for word in her_tokens]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distances among texts in vector space\n",
    "\n",
    "from sklearn.metrics import pairwise\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multi-Dimensional Scaling\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fussing with matplotlib\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q. What kinds of semantic relationships exist in the diagram above?\n",
    "##    Are there any words that seem out of place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving/Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save current model for later use\n",
    "model.save_word2vec_format('word2vec.txtalb_Novel150_English.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up models from disk\n",
    "\n",
    "# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n",
    "# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n",
    "\n",
    "ecco_model = gensim.models.Word2Vec.load_word2vec_format('word2vec.ECCO-TCP.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model trained on Reuters news articles\n",
    "reuters_model = gensim.models.Word2Vec.load_word2vec_format('word2vec.reuters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n",
    "##     Riches are to Virtue what Learning is to Genius. How true is this in\n",
    "##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n",
    "\n",
    "##  Q. How might we compare word2vec models more generally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word2Vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "The Word2Vec algorithm is a clever implementation of a more general Neural Network.\n",
    "\n",
    "Neural Networks can be thought of as <i>function approximators</i>. Oftentimes, we have inputs and outputs that can be represented as vectors but do not know the function that transforms input to output even though we would like to reproduce it. For example, if we throw a ball directly upward, we already know based on Newtonian physics that its height will be a function of time as determined by gravity and the initial velocity, and that we can identify its path in terms of these. However, without knowing anything about these features of motion, a Neural Network could be used to reverse engineer the path of the ball by showing it a series of inputs (times) and their associated outputs (heights). In turn, the Neural Network would be able to predict the ball's height at later times or in between the times it had been shown.\n",
    "\n",
    "Neural Networks are very often used to approximate functions that identify whether images have cats in them.\n",
    "\n",
    "At the grittiest mathematical level, Neural Networks consist of (at least!) two matrices. For convenience, we'll call them <i>Matrix A</i> and <i>Matrix B</i>. Without going into the Linear Algebra, the input vector is initially multiplied by Matrix A, creating a new vector that is then multiplied by Matrix B, producing the network's output vector. And that's it. The key here are the values contained in each of the matrices.\n",
    "\n",
    "The values of the network's matrices are learned through an iterative, correction process. Initially, the matrices are assigned random values, so naturally the network's output is random as well. However, that intial output gets compared to a desired, target vector and based on its error, the matrices get tweaked. In our previous example, the network would have been trained by showing it a given time (say, 2 seconds after throwing the ball) and comparing its predicted output (say, a randomly chosen -127 feet above ground) to the target output (say, 11 feet above ground). The values of Matrix B would first need to be revised in order to produce the correct output and the values of Matrix A would be revised based on those of Matrix B. Further pairs of inputs and outputs would be shown to the Network and the values of each matrix further updated.\n",
    "\n",
    "Once the network has been trained, we might think of Matrix A as encoding relationships among inputs and Matrix B as decoding these into outputs. Word2Vec takes advantage of the fact that when we train a Neural Network on CBOW or Skip-Gram models of language, each line of Matrix A represents a unique word's projection into semantic space.\n",
    "\n",
    "### A Note About Terminology\n",
    "I have used my own terms to describe the structure of a Neural Network, since they are simpler for our particular application. Networks are often described in terms of Layers, Nodes, and Weights.\n",
    "\n",
    "The input vector might be referred to as Layer 0, or the input layer. The vector that is produced by multiplying the input vector by Matrix A is Layer 1, or more often, the hidden layer. And the vector that is produced after multiplication with Matrix B is Layer 2, or the output layer. The network I have described has just two layers, but there may be potentially very many -- i.e. many hidden layers between the input and output. This is referred to as <i>deep learning</i>.\n",
    "\n",
    "Matrices A and B are typically referred to as Weight Matrices. The number of dimensions in them are referred to as nodes. The idea is that each node in each layer is connected to each node in the next one, akin to the neurons of a human brain. The matrices record the strength of connection between each node.\n",
    "\n",
    "### One-Hot Vector Representation of Words\n",
    "In order to make useful inputs and targets for our Neural Network, words initially get represented by a <i>one-hot vector</i>. Essentially, the one-hot encoding of a word is a document-term matrix for a single document with a single word and as many columns as there are unique words in the entire corpus. Every value for the document is zero except in the column for the word in question. This strategy is used to distinguish words from one another when they are represented as inputs and targets. Since the values of the vector are entirely independent of one another, no relationships between words get expressed by these.\n",
    "\n",
    "For CBOW, the target output is simply the one-hot vector of the word in question. The input is the average of the one-hot vectors representing context words within a given window.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Vocabulary: Most frequent words to include in our vocabulary; determines size of one-hot word vectors and, in turn, one dimensions of  matrices</li>\n",
    "<li>Semantic Space: Dimensions in which words' relationships get encoded; determines other dimension of matrices; alternately referred to as the number of <i>hidden nodes</i></li>\n",
    "<li>Window: Context words to observe left and right</li>\n",
    "<li>Alpha: learning rate; prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Epochs: Passes through dataset</li>\n",
    "<li>Batch Size: Words to sample from data during each pass</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set value for each feature\n",
    "\n",
    "vocab_size = 10000\n",
    "semantic_space_size = 100\n",
    "window = 5\n",
    "alpha = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign each unique token to a column in the one-hot vector\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = [token for sentence in words_by_sentence for token in sentence]\n",
    "token_counts = Counter(all_tokens)\n",
    "common_tokens = [token for token,frequency in token_counts.most_common(vocab_size-1)]\n",
    "\n",
    "# We'll add a dummy term to represent all infrequent words\n",
    "common_tokens.append('_SLUG_')\n",
    "\n",
    "token_to_ix = { tk:i for i,tk in enumerate(common_tokens) }\n",
    "ix_to_token = { i:tk for i,tk in enumerate(common_tokens) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "token_to_ix['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "ix_to_token[1618]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize matrices with random values\n",
    "\n",
    "Matrix_A = np.random.randn(vocab_size,semantic_space_size)*0.01\n",
    "Matrix_B = np.random.randn(semantic_space_size,vocab_size)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "Matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need two functions:\n",
    "# 1) Produce appropriate input and output vectors for our CBOW model, given a list of tokens in a sentence\n",
    "# 2) Update the values in our matrices after each observation\n",
    "\n",
    "def CBOW_input_output(sentence):\n",
    "    \n",
    "    # Get one-hot values for each word in sentence\n",
    "    ix_map = [token_to_ix[word] if word in common_tokens else token_to_ix['_SLUG_'] for word in sentence]\n",
    "    \n",
    "    # Randomly select a target word\n",
    "    target_index = np.random.choice(range(len(sentence)))\n",
    "    \n",
    "    \n",
    "    # Create (target) vector in which all values are zero\n",
    "    target_vector = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Set target-word value to 1\n",
    "    target_vector[ix_map[target_index]] = 1\n",
    "    \n",
    "    \n",
    "    # Create (input) vector in which all values are zero\n",
    "    input_vector = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Iterate through each of window-distance words to left of target\n",
    "    for i in range(target_index-window, target_index):\n",
    "        \n",
    "        # Add 1 to each word's value in the input vector\n",
    "        if i in range(len(sentence)) and sentence[i] in common_tokens:\n",
    "            input_vector[ix_map[i]] += 1\n",
    "\n",
    "            \n",
    "    # Iterate through each of window-distance words to left of target\n",
    "    for i in range(target_index+1, target_index+window+1):\n",
    "        \n",
    "        # Add 1 to each word's value in the input vector\n",
    "        if i in range(len(sentence)) and sentence[i] in common_tokens:\n",
    "            input_vector[ix_map[i]] += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    # Divide by number of words observed\n",
    "    input_vector = input_vector/(sum(input_vector)+1e-8)\n",
    "            \n",
    "    return input_vector, target_vector\n",
    "\n",
    "\n",
    "\n",
    "def train_function(input_vector_, target_vector_, Matrix_A_, Matrix_B_, alpha_):\n",
    "    \n",
    "    # Multiply input by matrices to produce output\n",
    "    hidden_vector = np.dot(input_vector_.T,Matrix_A_)\n",
    "    output_vector = 1/(1+np.exp(-(np.dot(hidden_vector,Matrix_B_))))\n",
    "    \n",
    "    # Note that after the output vector is produced, it is put into an activation function\n",
    "    # -- in this case, the logistic function -- that squashes its values between (0,1).\n",
    "    # This basically measures each vocabulary word's probability of being the true output.\n",
    "    \n",
    "    # Determine error for each matrix\n",
    "    output_delta = (target_vector_.T - output_vector)\n",
    "    hidden_delta =  np.dot(output_delta,Matrix_B_.T)\n",
    "\n",
    "    # Update matrix values\n",
    "    Matrix_B_ += np.dot(hidden_vector.T,output_delta)*alpha_\n",
    "    Matrix_A_ += np.dot(input_vector_,hidden_delta)*alpha_\n",
    "    \n",
    "    return(Matrix_A_,Matrix_B_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train our Neural Network\n",
    "\n",
    "for _ in range(epochs):\n",
    "    sampled_sentences = np.random.choice(words_by_sentence, batch_size)\n",
    "    for sentence in sampled_sentences:\n",
    "        input_vector, target_vector = CBOW_input_output(sentence)\n",
    "        Matrix_A, Matrix_B = train_function(input_vector, target_vector, Matrix_A, Matrix_B, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "Matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize each matrix row\n",
    "\n",
    "row_norm = np.linalg.norm(Matrix_A, axis=1)\n",
    "Matrix_A_norm = Matrix_A / row_norm[:, numpy.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create functions to retrieve word embeddings, determine similarities\n",
    "\n",
    "def get_vector(word,embedding_matrix=Matrix_A_norm):\n",
    "    if word in common_tokens:\n",
    "        ix = token_to_ix[word]\n",
    "        return embedding_matrix[ix]\n",
    "    else:\n",
    "        print(word, \"not in vocabulary\")\n",
    "    \n",
    "\n",
    "def token_similarity(token_1,token_2):\n",
    "    vector_1 = get_vector(token_1)\n",
    "    vector_2 = get_vector(token_2)\n",
    "    return 1-cosine(vector_1,vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve word embedding\n",
    "get_vector('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pronouns should be close to one another\n",
    "token_similarity('she','her')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compare a pronoun to a control word with a different POS\n",
    "token_similarity('call','me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can we recreate the analogy 'king' - 'man' + 'woman' ~ 'queen?\n",
    "# This formulation plays with some of the underlying algebra\n",
    "\n",
    "token_similarity('king','queen') - token_similarity('man','queen') + token_similarity('king','queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Write a function that will produce inputs and outputs for the Skip-Gram model.\n",
    "\n",
    "## EX. Our model randomly selects words from the corpus, so it implicitly prefers frequent tokens.\n",
    "##     Modify the script to minimize (but not eliminate) appearances of high-frequency terms.\n",
    "##     as components of input vectors.\n",
    "\n",
    "## EX. It has been found that the learning process can be sped up by showing \"wrong\" targets\n",
    "##     during training and penalizing the network when it predicts them as outputs.\n",
    "##     Implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Vector Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create functions that reject vectors, find nearest words\n",
    "\n",
    "def vector_rejector(vector_1,vector_2):\n",
    "    # Rejects vector_2 from vector_1\n",
    "    # Note that vectors are passed in, not words; also mean of\n",
    "    # several vectors could be passed as a single argument\n",
    "    vector_2_norm = vector_2 / np.linalg.norm(vector_2)\n",
    "    projection = np.dot(vector_1,vector_2_norm) * vector_2_norm\n",
    "    return vector_1 - projection\n",
    "\n",
    "def nearest_words(vector,embedding_matrix=Matrix_A_norm):\n",
    "    # Note that this relies on our 'ix_to_token' dictionary;\n",
    "    # format of embedding matrix is 2-D numpy array\n",
    "    new_matrix = np.dot(embedding_matrix, vector)\n",
    "    nearest_rows  = gensim.matutils.argsort(new_matrix, topn = 10, reverse = True)\n",
    "    result = [(ix_to_token[row_id], float(new_matrix[row_id])) for row_id in nearest_rows]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "she_vector = vector_rejector(get_vector('she'),get_vector('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "she_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_words(she_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

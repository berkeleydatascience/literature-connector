{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text Processing\n",
    "<li>Tokenize</li>\n",
    "<li>Stemming</li>\n",
    "\n",
    "## Tagging Words\n",
    "<li>Part of Speech</li>\n",
    "<li>Named Entity Recognition</li>\n",
    "\n",
    "## Lexical Resources\n",
    "<li>CMU Pronouncing Dictionary</li>\n",
    "<li>WordNet</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the Natural Language Toolkit\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: If NLTK is installed, but not 'nltk_data'\n",
    "\n",
    "modules = [\"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"punkt\",\\\n",
    "           \"words\", \"cmudict\", \"wordnet\"]\n",
    "\n",
    "for module in modules:\n",
    "    nltk.download(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read file as a string for later use\n",
    "\n",
    "with open('Melville - Moby Dick.txt') as file_in:\n",
    "    moby_text = file_in.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = \"What... is the air-speed velocity of an unladen swallow? \\\n",
    "What do you mean? An African or European swallow?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(sent) for sent in sent_tokenize(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Use the function word_tokenize() in order to tokenize Moby Dick.\n",
    "##     How many tokens does the novel contain?\n",
    "\n",
    "## EX. In previous lessons we have used the .split() method in order\n",
    "##     to tokenize texts.\n",
    "##     If you use that method, how many tokens do we find in Moby Dick?\n",
    "##     Why is that number different from the previous exercise?\n",
    "\n",
    "## EX. How many sentences are there in Moby Dick?\n",
    "\n",
    "## Challenge: What is the average number of tokens per sentence in Moby Dick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common stemming algorithms\n",
    "# Note: Snowball is sometimes also called the 'Porter' algorithm\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball  import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "english_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_stemmer.stem('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_stemmer.stem('admirably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl.lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WordNet POS tags\n",
    "\n",
    "# VRB  >> 'v'\n",
    "# ADJ  >> 'a'\n",
    "# NOUN >> 'n'\n",
    "# ADV  >> 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. Create a list of stemmed words from Moby Dick.\n",
    "\n",
    "## Challenge: How many unique tokens are there in Moby Dick? Unique stems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Common POS taggers\n",
    "\n",
    "# Note: Stanford models must be downloaded from here: http://nlp.stanford.edu/software/\n",
    "# NLTK simply offers a wrapper for the Stanford taggers, which allows\n",
    "# you to use them in Python, rather than their native Java\n",
    "\n",
    "from nltk.tag.perceptron    import PerceptronTagger\n",
    "from nltk.tag.brill         import BrillTagger\n",
    "from nltk.tag.stanford      import StanfordTagger, StanfordPOSTagger, StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK's current default POS tagger is the 'averaged perceptron' as described here:\n",
    "# https://spacy.io/blog/part-of-speech-POS-tagger-in-python\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "new_sentence = \"Once the number three, being the third number, be reached, \\\n",
    "then lobbest thou thy Holy Hand Grenade of Antioch towards thy foe, who, \\\n",
    "being naughty in My sight, shall snuff it.\"\n",
    "\n",
    "new_tokens = word_tokenize(new_sentence)\n",
    "\n",
    "pos_tag(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Penn Treebank POS Tags\n",
    "# www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    print(pos_tag(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Get POS tags for the sentence below.\n",
    "\n",
    "very_new_sentence = \"On second thought, let's not go to Camelot.\"\n",
    "\n",
    "## EX. Rewrite the last for-loop above (over 'sentences') as a list comprehension.\n",
    "\n",
    "## Challenge: Create a list containing only the nouns from 'very_new_sentence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's start with a fresh sentence containing several proper names\n",
    "\n",
    "ner_sentence = 'King Arthur is the sovereign over Britain and lord of the Round Table.'\n",
    "ner_tokens = word_tokenize(ner_sentence)\n",
    "ner_tags = pos_tag(ner_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "chunks = ne_chunk(ner_tags)\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "        if chunk.label()=='PERSON':\n",
    "            print(chunk.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. Retrieve the geograpic designations from the sentence below.\n",
    "\n",
    "swallow_skeptic = \"Oh yeah, an African swallow, maybe, but not a European swallow.\"\n",
    "\n",
    "## EX. Rewrite the previous exercise as a list comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Pronouncing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = cmudict.words()      # list of words for which we have pronunciations\n",
    "dictionary = cmudict.dict()  # keys are words, values are lists of pronunciations for each word\n",
    "entries = cmudict.entries()  # list of tuples, where first entry is word, second is pronunciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in words[42371:42379]:\n",
    "     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for entry in entries[42371:42379]:\n",
    "     print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary['fir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner_sentence = 'King Arthur is the sovereign over Britain and lord of the Round Table.'\n",
    "tokens = word_tokenize(ner_sentence.lower())\n",
    "for token in tokens:\n",
    "    if token in words:\n",
    "        print(token, dictionary[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Retrieve the pronunciation for the sentence below\n",
    "\n",
    "famous_sentence = \"To be or not to be that is the question.\"\n",
    "\n",
    "## Challenge: Count the number of syllables in the famous sentence\n",
    "\n",
    "## Challenge: What is the average number of syllables per line in Hamlet's soliloquy?\n",
    "## The text is contained in the file \"hamlet.txt\" in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wordnet-hierarchy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguity: Locating words in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that wn.synsets(word) takes a dictionary word as its entry\n",
    "# and returns the labels for each of its definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that wn.synset(label) simply represents the definition\n",
    "# corresponding to that label\n",
    "\n",
    "# The 's' at the end of the function name makes all the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.02').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.02').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('car'):\n",
    "     print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(wn.synsets('car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. How many synsets does the word 'swallow' belong to?\n",
    "##     What are their definitions?\n",
    "\n",
    "##  Q. Does the number of synsets to which a word belongs offer\n",
    "##     a useful measure of word ambiguity? In what situations\n",
    "##     might it come in handy? When might it be misleading?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstraction: How high is your tree branch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in wn.synset('car.n.01').hypernym_paths():\n",
    "    print(len(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  Q. Describe in words the relationship between a word and its hyponyms.\n",
    "##     Also, between the word and its hypernyms.\n",
    "\n",
    "## EX. Find the hypernyms and hyponyms for the definition 'swallow.n.03'\n",
    "\n",
    "## EX. How long is the hypernym path for the definition 'swallow.n.03'?\n",
    "##     What about 'coconut.n.02'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "april = \"\"\"THREE spirits came to me\n",
    "And drew me apart\n",
    "To where the olive boughs\n",
    "Lay stripped upon the ground:\n",
    "Pale carnage beneath bright mist.\"\"\"\n",
    "\n",
    "metro = \"\"\"The apparition of these faces in the crowd;\n",
    "Petals on a wet, black bough.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CHALLENGE: Measure the average number of possible synsets per word for each poem.\n",
    "##            Does this capture your intuition about the relative ambiguity of each?\n",
    "##            HINT: Use the WordNet Lemmatizer!\n",
    "\n",
    "##  Q: Is there a way to perform a similar measure of average hypernym_path length?\n",
    "##     How might you handle words for which there are multiple synsets with differing lengths?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

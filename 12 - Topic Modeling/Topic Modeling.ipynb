{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling in Python\n",
    "<ol><li>Prep</li>\n",
    "<li>Pre-Process</li>\n",
    "<li>Topic Model</li>\n",
    "<li>Interpreting the Model</li>\n",
    "<li>Revising Model Inputs</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import nltk\n",
    "modules = [\"punkt\", \"words\", \"stopwords\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\"]\n",
    "for module in modules:\n",
    "    nltk.download(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Corpus Description\n",
    "English-language subset of Andrew Piper's novel corpus, totaling 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n",
    "\n",
    "### Metadata Columns\n",
    "<ol><li>Filename: Name of file on disk</li>\n",
    "<li>ID: Unique ID in Piper corpus</li>\n",
    "<li>Language: Language of novel</li>\n",
    "<li>Date: Initial publication date</li>\n",
    "<li>Title: Title of novel</li>\n",
    "<li>Gender: Authorial gender</li>\n",
    "<li>Person: Textual perspective</li>\n",
    "<li>Length: Number of tokens in novel</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read metadata\n",
    "metadata_tb = Table.read_table('txtlab_Novel150_English.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set location of corpus folder\n",
    "fiction_path = 'txtalb_Novel150_English/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read novel plaintext from file\n",
    "\n",
    "# Create empty list, entries will be list of tokens from each novel\n",
    "novel_list = []\n",
    "\n",
    "# Iterate through filenames in metadata table\n",
    "for filename in metadata_tb['filename']:\n",
    "    \n",
    "    # Read in novel text as single string, make lowercase\n",
    "    with open(fiction_path+filename, 'r') as file_in:\n",
    "        novel = file_in.read()\n",
    "    \n",
    "    # Add list of tokens to master list\n",
    "    novel_list.append(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-Process\n",
    "\n",
    "Typically, this is the point where we would process texts into a document-term matrix. In this case, our workflow is tailored to the topic-modeling package's format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Even though I love nltk's word_tokenize(), I just need a\n",
    "# quick and dirty tokenizer. Emphasis on quick.\n",
    "\n",
    "def fast_tokenizer(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    from string import punctuation\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in text if char not in punctuation])\n",
    "    \n",
    "    # Split text over whitespace\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare tokenizers on a test sentence\n",
    "\n",
    "wuthering_heights = \"1801.—I have just returned from a visit to my landlord—the solitary neighbour that I shall be troubled with.\"\n",
    "\n",
    "print(fast_tokenizer(wuthering_heights))\n",
    "print(nltk.word_tokenize(wuthering_heights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "noveltokens_list = [fast_tokenizer(novel.lower()) for novel in novel_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect tokens from first novel\n",
    "noveltokens_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Topic Model package\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary based on corpus tokens\n",
    "# Each token is mapped to its own unique ID\n",
    "\n",
    "dictionary = gensim.corpora.dictionary.Dictionary(noveltokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map lists of tokens to the dictionary IDs\n",
    "dictionary.doc2bow(['pride','prejudice', 'pride'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords, (some!) proper names from dictionary\n",
    "from nltk.corpus import stopwords, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proper name test\n",
    "'Ishmael' in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proper_names = [word.lower() for word in words.words() if word.istitle()]\n",
    "bad_words = stopwords.words('english')+proper_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map stopwords, proper names to dictionary IDs\n",
    "stop_ids = [_id for _id,count in dictionary.doc2bow(bad_words)]\n",
    "\n",
    "# Remove stopwords from dictionary mappings\n",
    "dictionary.filter_tokens(bad_ids = stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove terms by document frequency\n",
    "dictionary.filter_extremes(no_below=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of dictionary mappings by novel\n",
    "# This is gensim's version of a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in noveltokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect corpus element\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) Models\n",
    "LDA reflects an intuition that words in a text are not merely chosen at random but are drawn from underlying concepts (the so-called \"latent variables\"). The goal of LDA is to look across many texts in order to reverse engineer these concepts by finding words that tend to cluster with one another. For this reason, LDA has been referred to as \"the mother of all word collocation techniques.\"\n",
    "\n",
    "### Topic Model Features\n",
    "<ul><li>Corpus: Pre-processed textual corpus</li>\n",
    "<li>Number of Topics: Choosing this is the art of Topic Modeling </li>\n",
    "<li>Alpha (Hyperparameter): Prior reflecting expected distribution of topics over documents</li>\n",
    "<li>Iterations: TM initially uses random distribution, iteratively tweaks model</li>\n",
    "<li>Passes: Bootstrap method for evaluating model; primarily seen in Gensim implementation</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Topic Model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=40, alpha='auto', \\\n",
    "                                   id2word=dictionary, iterations=2500, passes = 4)\n",
    "\n",
    "# If you have more than two cores at your disposal, then perhaps try:\n",
    "#lda_model = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=40, id2word=dictionary, iterations=2500, passes = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick look at n topics among those inferred\n",
    "lda_model.show_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deeper look at particular topic\n",
    "lda_model.show_topic(8, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deeper look but labels with id2word mapping\n",
    "lda_model.get_topic_terms(8, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. The 'topn' argument returns only the given number of terms\n",
    "##     for each topic. Rewrite that argument to return all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Return a list of all term values for topic 0. By default,\n",
    "##     these are ordered by the probability associated with each term.\n",
    "##     Instead, order the list according to the words id2word label\n",
    "\n",
    "## CHALLENGE: Create a table that contains all topic-term distributions.\n",
    "##     Each row is a topic and each column is labelled by the word it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Measure of model's \"fit\" to data\n",
    "# Related to the probability of seeing text given inferred model\n",
    "\n",
    "lda_model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most present topics in corpus\n",
    "lda_model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Most prominent topics in a given document\n",
    "lda_model.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of all topics over a document\n",
    "lda_model.get_document_topics(corpus[0], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Return a list of the most prominent topics in document 10.\n",
    "##     What terms are most prominent in those topics?\n",
    "\n",
    "## EX. Compare your answers to the previous exercise with a classmate.\n",
    "##     Do similar topics come up? Different ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interpeting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "There are many strategies that can be used to interpret the output of a topic model. In this case, we will look for any correlations between the topic distributions and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of all document-topic distributions\n",
    "list_of_doctopics = [lda_model.get_document_topics(corpus[i], minimum_probability=0) for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_doctopics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the list above, each topic got represented as a tuple containing\n",
    "# the label of the topic and its probability within the given document\n",
    "\n",
    "# Create list containing only the probabilities (remains ordered by topic label)\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll put these into a labeled column format so that we can add\n",
    "# document-topic distributions to our original metadata table\n",
    "\n",
    "# Note that this means a cumbersome switch from lists that represent rows\n",
    "# to lists that represent columns\n",
    "\n",
    "labeled_columns = [['Topic '+str(i),[document[i] for document in list_of_probabilities]] for i in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add these as new columns to the metadata table\n",
    "metatopic_tb = metadata_tb.with_columns(labeled_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick and dirty correlation function\n",
    "\n",
    "def correlator(tb, col_1, col_2):\n",
    "    import numpy as np\n",
    "    col_1_in_su = [(x-np.mean(tb[col_1]))/np.std(tb[col_1]) for x in tb[col_1]]\n",
    "    col_2_in_su = [(x-np.mean(tb[col_2]))/np.std(tb[col_2]) for x in tb[col_2]]\n",
    "    col_mult = [col_1_in_su[i]*col_2_in_su[i] for i in range(len(col_1_in_su))]\n",
    "    r = np.mean(col_mult)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlator(metatopic_tb, 'date', 'Topic 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Find any topics that have an r^2 value greater than 0.1.\n",
    "##     Return the top terms for those topics. Are the correlations\n",
    "##     positive or negative?\n",
    "\n",
    "## EX. Try running the topic model without removing any words from\n",
    "##     the dictionary. How do the topics change?\n",
    "##                     Try changing the minimum document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Revising Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Some proper names and titles still came through our filter.\n",
    "##     Use nltk's NER function to remove names in a more targeted way.\n",
    "\n",
    "## EX. In Matt Jockers's study of literary theme, he included only\n",
    "##     nouns for topic modeling. Use nltk's POS tagger to remove all\n",
    "##     words from the corpus that are not common nouns.\n",
    "\n",
    "## EX. Jockers also found it useful to split texts into 1000-noun chunks\n",
    "##     after the POS filter. Run the topic model over these smaller chunks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

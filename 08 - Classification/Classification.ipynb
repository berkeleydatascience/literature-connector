{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to reproduce several findings from Ted Underwood and Jordan Sellers's article \"How Quickly Do Literary Standards Change?\" (draft (2015), forthcoming in <i>Modern Language Quarterly</i>). See especially Fig 1 and reported classifier accuracy (p 8).\n",
    "\n",
    "Underwood and Sellers have made their corpus of poems and their code available here: https://github.com/tedunderwood/paceofchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Intro to Logistic Regression\n",
    "<li>Intuiting the Linear Regression Model</li>\n",
    "<li>Intuiting the Logistic Regression Model</li>\n",
    "<li>Logistic Classification</li>\n",
    "\n",
    "## Cross-Validation\n",
    "<li>Pre-Processing</li>\n",
    "<li>Feature Selection, Training, Predictions</li>\n",
    "<li>Efficiency</li>\n",
    "<li>Evaluation</li>\n",
    "\n",
    "## Classification\n",
    "<li>The Canon</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Intro to Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from datascience import *\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dummy Data Set\n",
    "\n",
    "demo_tb = Table()\n",
    "\n",
    "demo_tb['Study Hours'] = [2.0, 6.9, 1.6, 7.8, 3.1, 5.8, 3.4, 8.5, 6.7, 1.6, 8.6, 3.4, 9.4, 5.6, 9.6, 3.2, 3.5, 5.9, 9.7, 6.5]\n",
    "demo_tb['Grade'] = [67.0, 83.6, 35.4, 79.2, 42.4, 98.2, 67.6, 84.0, 93.8, 64.4, 100.0, 61.6, 100.0, 98.4, 98.4, 41.8, 72.0, 48.6, 90.8, 100.0]\n",
    "demo_tb['Pass'] = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
    "\n",
    "demo_tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuiting the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input  >> Continuous Variable\n",
    "# Output >> Continuous Variable\n",
    "\n",
    "# Tries to find underlying linear relationship between inputs and outputs\n",
    "\n",
    "demo_tb.scatter('Study Hours','Grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_tb.scatter('Study Hours','Grade', fit_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuiting the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input  >> Continuous Variable\n",
    "# Output >> Categorical Variable\n",
    "\n",
    "# Tries to find probability of an output given an input\n",
    "\n",
    "demo_tb.scatter('Study Hours','Pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the Logistic Function\n",
    "\n",
    "def logistic(p):\n",
    "    return 1/(1+exp(-p))\n",
    "\n",
    "# Assign generic Logistic Regression Coefficients\n",
    "\n",
    "B0, B1 = 0,1\n",
    "\n",
    "# Clunky Plotting for Continuous Functions\n",
    "\n",
    "points = int(1e4)\n",
    "xmin, xmax = -10,10\n",
    "xlist = [float(x)/points for x in range(xmin*points, xmax*points)]\n",
    "ylist = [logistic(B0 + B1*x) for x in xlist]\n",
    "\n",
    "axis([-10, 10, -0.1,1.1])\n",
    "plot(xlist,ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Logistic Regression Coefficients\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(demo_tb['Study Hours'].reshape(-1,1),demo_tb['Pass'].reshape(20,))\n",
    "B0, B1 = lr.intercept_[0], lr.coef_[0]\n",
    "\n",
    "# Clunky Plotting for Continuous Functions\n",
    "\n",
    "points = int(1e4)\n",
    "xmin, xmax = 0,10\n",
    "xlist = [float(x)/points for x in range(xmin*points, xmax*points)]\n",
    "ylist = [logistic(B0 + B1*x) for x in xlist]\n",
    "plot(xlist,ylist)\n",
    "\n",
    "# Add our \"Observed\" Data Points\n",
    "\n",
    "scatter(demo_tb['Study Hours'],demo_tb['Pass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "\n",
    "train_hours = demo_tb.column('Study Hours')[:-2]\n",
    "train_targets = demo_tb.column('Pass')[:-2]\n",
    "\n",
    "test_hours = demo_tb.column('Study Hours')[-2:]\n",
    "test_targets = demo_tb.column('Pass')[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the values we are using to validate the model\n",
    "\n",
    "print(test_hours, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Logistic Regression Coefficients\n",
    "\n",
    "lr.fit(train_hours.reshape(-1,1),train_targets.reshape(len(train_targets),))\n",
    "B0, B1 = lr.intercept_[0], lr.coef_[0]\n",
    "\n",
    "# Determing the probability each student would pass, based on our model\n",
    "fitted = [logistic(B1*th + B0) for th in test_hours]\n",
    "\n",
    "# See whether that probability is greater than 50% (i.e. the most likely outcome)\n",
    "prediction = [pred>.5 for pred in fitted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test each prediction against the true answer in the validation set\n",
    "\n",
    "prediction_eval = [prediction[i]==test_targets[i] for i in range(len(prediction))]\n",
    "float(sum(prediction_eval)/len(prediction_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Put the values from the \"Study Hours\" column into standard units.\n",
    "\n",
    "# EX. Reproduce the Logistic Regression graph using \"observed\" values in\n",
    "#     standard units.\n",
    "\n",
    "#  Q. In the previous exercise, what does the new graph represent?\n",
    "#     What is happening at x = 0 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "corpus_path = 'poems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#metadata_tb = pd.read_csv('poemeta.csv')\n",
    "metadata_tb = Table.read_table('poemeta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the records that we will eventually try to classify\n",
    "\n",
    "reception_mask = (metadata_tb['recept']=='reviewed') + (metadata_tb['recept']=='random')\n",
    "clf_tb = metadata_tb.where(reception_mask)\n",
    "\n",
    "clf_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Term Frequencies by Text\n",
    "\n",
    "\n",
    "# NOTE: This script is specifically tailored to the format of \n",
    "# textual data made available from Hathi Trust. This consists of a \n",
    "# series ofspreadsheets, each containing book-level term frequencies.\n",
    "\n",
    "# Each spreadsheet will become a row in our Document-Term Matrix.\n",
    "\n",
    "\n",
    "# Create list that will contain a series of dictionaries\n",
    "freqdict_list = []\n",
    "\n",
    "# Iterate through texts in our spreadsheet\n",
    "for _id in clf_tb['docid']:\n",
    "\n",
    "    # Each text will have its own dictionary\n",
    "    # Keys are terms and values are frequencies\n",
    "    termfreq_dict = {}\n",
    "    \n",
    "    # Open the given text's spreadsheet\n",
    "    with open(corpus_path+_id+'.poe.tsv', encoding='utf-8') as file_in:\n",
    "        filelines = file_in.readlines()\n",
    "        \n",
    "        # Each line in the spreadsheet contains a unique term and its frequency\n",
    "        for line in filelines:\n",
    "            termfreq = line.split('\\t')\n",
    "            \n",
    "            # 'If' conditions throw out junk lines in the spreadsheet\n",
    "            if len(termfreq) > 2 or len(termfreq) > 2:\n",
    "                continue\n",
    "            term, freq = termfreq[0], int(termfreq[1])\n",
    "            if len(term)>0 and term[0].isalpha():\n",
    "                \n",
    "                # Create new entry in text's dictionary for the term\n",
    "                termfreq_dict[term] = freq\n",
    "                \n",
    "    freqdict_list.append(termfreq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert our list of term-frequency dictionaries into a document-term matrix\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer()\n",
    "dtm = dv.fit_transform(freqdict_list)\n",
    "term_list = dv.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put our Document-Term-Matrix into human-readable format\n",
    "\n",
    "dtm_tb = Table(term_list).with_rows(dtm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection, Training, Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas DataFrame -- not Table -- since offers additional methods\n",
    "\n",
    "import pandas as pd\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the unique IDs from our metadata to keep track of each text\n",
    "\n",
    "dtm_df.set_index(clf_tb['docid'], inplace=True)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Underwood and Sellers create a unique model for each author in the corpus.\n",
    "# They set aside a given author from the training set and then use the model to\n",
    "# predict whether she was likely to be reviewed or not.\n",
    "\n",
    "# Create a list of authors and an \"empty\" array in which to record probabilities\n",
    "\n",
    "authors = list(set(clf_tb['author']))\n",
    "probabilities = np.zeros([len(clf_tb['docid'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Logistic Regressor\n",
    "\n",
    "# Underwood and Sellers use a regularization constant ('C') to prevent overfitting,\n",
    "# since this is a major concern when observing thousands of variables.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C = 0.00007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember: Each author's model potentially consists of a unique set of 3200 words.\n",
    "\n",
    "\n",
    "# Set aside each author's texts from training set\n",
    "\n",
    "def set_author_aside(author, tb, df):\n",
    "    train_ids = tb.where(tb['author']!=author).column('docid')\n",
    "    test_ids = tb.where(tb['author']==author).column('docid')\n",
    "    \n",
    "    train_df_ = df.loc[train_ids]\n",
    "    test_df_ = df.loc[test_ids]\n",
    "    \n",
    "    train_targets_ = tb.where(tb['author']!=author)['recept']=='reviewed'\n",
    "    \n",
    "    return train_df_, test_df_, train_targets_\n",
    "\n",
    "\n",
    "# Retrieve the most common words (by document frequency) for a given model\n",
    "\n",
    "def top_vocab_by_docfreq(df, num_words):\n",
    "    docfreq_df = df > 0\n",
    "    wordcolumn_sums = sum(docfreq_df)\n",
    "    words_by_freq = wordcolumn_sums.sort_values(ascending=False)\n",
    "    top_words = words_by_freq[:num_words]\n",
    "    top_words_list = top_words.index.tolist()\n",
    "    \n",
    "    return top_words_list\n",
    "\n",
    "\n",
    "# Normalize the model's term frequencies and put them into standard units\n",
    "\n",
    "def normalize_model(train_df_, test_df_, vocabulary):\n",
    "    # Select columns for only the most common words\n",
    "    train_df_ = train_df_[vocabulary]\n",
    "    test_df_ = test_df_[vocabulary]\n",
    "    \n",
    "    # Normalize each value by the sum of all values in its row\n",
    "    train_df_ = train_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    test_df_ = test_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    \n",
    "    # Get mean and stdev for each column\n",
    "    train_mean = np.mean(train_df_)\n",
    "    train_std = np.std(train_df_)\n",
    "\n",
    "    # Transform each value to standard units for its column\n",
    "    train_df_ = ( train_df_ - train_mean ) / train_std\n",
    "    test_df_ = ( test_df_ - train_mean ) / train_std\n",
    "    \n",
    "    return train_df_, test_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Create a new function that determines top vocabulary by each word's\n",
    "##     total frequency in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to time this to make a point about computational efficiency\n",
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "for author in authors[:10]:\n",
    "    \n",
    "    # Set aside each author's texts from training set\n",
    "    train_df, test_df, train_targets = set_author_aside(author, clf_tb, dtm_df)\n",
    "\n",
    "    # Retrieve the most common words (by document frequency) for a given model\n",
    "    vocab_list = top_vocab_by_docfreq(train_df, 3200)\n",
    "    \n",
    "    # Normalize the model's term frequencies and put them into standard units\n",
    "    train_df, test_df = normalize_model(train_df, test_df, vocab_list)\n",
    "    \n",
    "    # Learn the Logistic Regression over our model\n",
    "    clf.fit(train_df, train_targets)\n",
    "    \n",
    "    # Some authors have more than one text in the corpus, so we retrieve all\n",
    "    for _id in test_df.index.tolist():\n",
    "        \n",
    "        # Make prediction whether text was reviewed\n",
    "        text = test_df.loc[_id]\n",
    "        probability = clf.predict_proba([text])[0][1]\n",
    "\n",
    "        # Record predictions in same order as the metadata spreadsheet\n",
    "        _index = list(clf_tb.column('docid')).index(_id)\n",
    "        probabilities[_index] = probability\n",
    "    \n",
    "        \n",
    "end = time.clock()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  Q. Why are we taking such pains to determine our vocabulary list\n",
    "##     without using texts from the test set? Would it make a big difference?\n",
    "\n",
    "##  Q. How can we make this process more efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-Processed Vocabulary\n",
    "# Contains only words that will be used in classification\n",
    "\n",
    "# This list was created by simply iterating through each model\n",
    "# and observing the words that appeared in it\n",
    "\n",
    "import pickle\n",
    "with open('preprocessed_vocab.pickle', 'rb') as f:\n",
    "    pp_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas DataFrame -- not Table -- since offers additional methods\n",
    "\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select only columns for words in our pre-processed vocabulary\n",
    "# This will make our computation more efficient later\n",
    "\n",
    "# dtm_tb = dtm_tb.select(pp_vocab)\n",
    "dtm_df = dtm_df[pp_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the unique IDs from our metadata to keep track of each text\n",
    "\n",
    "dtm_df.set_index(clf_tb['docid'], inplace=True)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Using code from our previous lessons, remove stopwords from our pre-processed vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new DataFrame that simply lists whether a term appears in\n",
    "# each document, so that we don't have to repeat this process evey iteration\n",
    "\n",
    "term_in_doc_df = dtm_df>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_in_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-write the model-building function\n",
    "\n",
    "def set_author_aside(author, tb, dtm_df_, dfreq_df_):\n",
    "    train_ids = tb.where(tb['author']!=author).column('docid')\n",
    "    test_ids = tb.where(tb['author']==author).column('docid')\n",
    "    \n",
    "    train_df_ = dtm_df_.loc[train_ids]\n",
    "    dfreq_df_ = dfreq_df_.loc[train_ids] # Include only term_in_doc values for texts in training set\n",
    "    test_df_ = dtm_df_.loc[test_ids]\n",
    "    \n",
    "    train_targets_ = tb.where(tb['author']!=author)['recept']=='reviewed'\n",
    "    \n",
    "    return train_df_, test_df_, train_targets_, dfreq_df_\n",
    "\n",
    "\n",
    "# Re-write our vocabulary selection function\n",
    "\n",
    "def top_vocab_by_docfreq(df, num_words):\n",
    "    # Removed the test of whether a term is in a given document (i.e. df>0)\n",
    "    wordcolumn_sums = sum(df)\n",
    "    words_by_freq = wordcolumn_sums.sort_values(ascending=False)\n",
    "    top_words = words_by_freq[:num_words]\n",
    "    top_words_list = top_words.index.tolist()\n",
    "    \n",
    "    return top_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C = 0.00007)\n",
    "\n",
    "def master_function(author):\n",
    "    # Note: Our only input is the name of the author.\n",
    "    # Remember that we had iterated over the list of authors previously.\n",
    "    train_df, test_df, train_targets, dfreq_df = set_author_aside(author, clf_tb, dtm_df, term_in_doc_df)\n",
    "    vocab_list = top_vocab_by_docfreq(dfreq_df, 3200)\n",
    "    train_df, test_df = normalize_model(train_df, test_df, vocab_list)\n",
    "    clf.fit(train_df, train_targets)\n",
    "    \n",
    "    # Create a list of each text's probability of review AND its index in the metadata table\n",
    "    index_probability_tuples = []\n",
    "    for _id in test_df.index.tolist():\n",
    "        text = test_df.loc[_id]\n",
    "        probability = clf.predict_proba([text])[0][1]\n",
    "        _index = list(clf_tb.column('docid')).index(_id)\n",
    "        index_probability_tuples.append( (_index, probability) )\n",
    "    return index_probability_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiprocessing enables Python to run its script on multiple cores simultaneously\n",
    "# This is best used in situations where we might otherwise use a 'FOR' loop.\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# Return number of cores\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Pool contains one worker for each core\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Efficiently applies the master_function() to our list of authors\n",
    "# Returns a list where each entry is an item returned by the function\n",
    "output = pool.map(master_function, authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each element in output is itself a list, the length of which is the number\n",
    "# of texts by a given author. We'll flatten it for ease of use.\n",
    "\n",
    "flat_output = [tup  for lst in output  for tup in lst]\n",
    "flat_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the indices returned with the output to arrange probabilities properly\n",
    "\n",
    "probabilities = np.zeros([len(clf_tb['docid'])])\n",
    "for tup in flat_output:\n",
    "    probabilities[tup[0]] = tup[1]\n",
    "\n",
    "clf_tb['P(reviewed)'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_tb.select(['docid', 'firstpub','author', 'title', 'recept', 'P(reviewed)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the probability each text was reviewed\n",
    "\n",
    "colors = ['r' if recept=='reviewed' else 'b'  for recept in clf_tb['recept']]\n",
    "\n",
    "clf_tb.scatter('firstpub', 'P(reviewed)', c=colors, fit_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Does the Logistic Regression Model think its likely each book was reviewed?\n",
    "predictions = probabilities>0.5\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creates array where '1' indicates a reviewed book and '0' indicates not\n",
    "targets = clf_tb['recept']=='reviewed'\n",
    "\n",
    "print(accuracy_score(predictions, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: Often we prefer to evaluate accuracy based on the F1-score, which\n",
    "# weighs the number of times we correctly predicted reviewed texts against\n",
    "# the number of times we incorrectly predicted them as 'random'.\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(predictions, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Change the regularization parameter ('C') in our Logistic Regression function.\n",
    "##     How does this change the classifier's accuracy?\n",
    "\n",
    "## EX. Reduce the size of the vocabulary used for classification. How does accuracy change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  Q. Are there cases when we might not want to set the classification threshold\n",
    "##     to 50% likelihood? How certain are we that 51% is different from a 49% probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model using full set of 'reviewed' and 'random' texts\n",
    "\n",
    "# Use this to predict the probability that other prestigious texts\n",
    "# (i.e. ones that we haven't trained on) might have been reviewed\n",
    "# at their times of publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Re-run script from scratch\n",
    "\n",
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from datascience import *\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "corpus_path = 'poems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read metadata from spreadsheet\n",
    "metadata_tb = Table.read_table('poemeta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll copy just our new texts into a separate table as well, for later\n",
    "canon_tb = metadata_tb.where('recept','addcanon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Term Frequencies from files\n",
    "\n",
    "freqdict_list = []\n",
    "\n",
    "# Iterate through texts in our spreadsheet\n",
    "for _id in metadata_tb['docid']:\n",
    "\n",
    "    # Each text will have its own dictionary\n",
    "    # Keys are terms and values are frequencies\n",
    "    termfreq_dict = {}\n",
    "    \n",
    "    # Open the given text's spreadsheet\n",
    "    with open(corpus_path+_id+'.poe.tsv', encoding='utf-8') as file_in:\n",
    "        filelines = file_in.readlines()\n",
    "        \n",
    "        # Each line in the spreadsheet contains a unique term and its frequency\n",
    "        for line in filelines:\n",
    "            termfreq = line.split('\\t')\n",
    "            \n",
    "            # 'If' conditions throw out junk lines in the spreadsheet\n",
    "            if len(termfreq) > 2 or len(termfreq) > 2:\n",
    "                continue\n",
    "            term, freq = termfreq[0], int(termfreq[1])\n",
    "            if len(term)>0 and term[0].isalpha():\n",
    "                \n",
    "                # Create new entry in text's dictionary for the term\n",
    "                termfreq_dict[term] = freq\n",
    "                \n",
    "    freqdict_list.append(termfreq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Document-Term-Matrix\n",
    "\n",
    "dv = DictVectorizer()\n",
    "dtm = dv.fit_transform(freqdict_list)\n",
    "term_list = dv.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Place the DTM into a Pandas DataFrame for further manipulation\n",
    "\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = term_list)\n",
    "dtm_df.set_index(metadata_tb['docid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are Feature Selection functions like the ones we originally defined,\n",
    "# not their efficiency minded counterparts, since we only train once\n",
    "\n",
    "\n",
    "# Set aside each canonic texts from training set\n",
    "\n",
    "def set_canon_aside(tb, df):\n",
    "    train_ids = tb.where(tb['recept']!='addcanon').column('docid')\n",
    "    classify_ids = tb.where(tb['recept']=='addcanon').column('docid')\n",
    "    \n",
    "    train_df_ = df.loc[train_ids]\n",
    "    classify_df_ = df.loc[classify_ids]\n",
    "    \n",
    "    train_targets_ = tb.where(tb['recept']!='addcanon')['recept']=='reviewed'\n",
    "    \n",
    "    return train_df_, classify_df_, train_targets_\n",
    "\n",
    "\n",
    "# Retrieve the most common words (by document frequency) for a given model\n",
    "\n",
    "def top_vocab_by_docfreq(df, num_words):\n",
    "    docfreq_df = df > 0\n",
    "    wordcolumn_sums = sum(docfreq_df)\n",
    "    words_by_freq = wordcolumn_sums.sort_values(ascending=False)\n",
    "    top_words = words_by_freq[:num_words]\n",
    "    top_words_list = top_words.index.tolist()\n",
    "    \n",
    "    return top_words_list\n",
    "\n",
    "\n",
    "# Normalize the model's term frequencies and put them into standard units\n",
    "\n",
    "def normalize_model(train_df_, classify_df_, vocabulary):\n",
    "    # Select columns for only the most common words\n",
    "    train_df_ = train_df_[vocabulary]\n",
    "    classify_df_ = classify_df_[vocabulary]\n",
    "    \n",
    "    # Normalize each value by the sum of all values in its row\n",
    "    train_df_ = train_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    classify_df_ = classify_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    \n",
    "    # Get mean and stdev for each column\n",
    "    train_mean = np.mean(train_df_)\n",
    "    train_std = np.std(train_df_)\n",
    "\n",
    "    # Transform each value to standard units for its column\n",
    "    train_df_ = ( train_df_ - train_mean ) / train_std\n",
    "    classify_df_ = ( classify_df_ - train_mean ) / train_std\n",
    "    \n",
    "    return train_df_, classify_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train our Logistic Regression Model\n",
    "\n",
    "clf = LogisticRegression(C = 0.00007)\n",
    "\n",
    "model_df, classify_df, model_targets = set_canon_aside(metadata_tb, dtm_df)\n",
    "vocab_list = top_vocab_by_docfreq(model_df, 3200)\n",
    "model_df, classify_df = normalize_model(model_df, classify_df, vocab_list)\n",
    "clf.fit(model_df, model_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict whether our new prestigious texts might have been reviewed\n",
    "\n",
    "probabilities = numpy.zeros([len(canon_tb.column('docid'))])\n",
    "for _id in classify_df.index.tolist():\n",
    "    text = classify_df.loc[_id]\n",
    "    probability = clf.predict_proba([text])[0][1]\n",
    "    \n",
    "    _index = list(canon_tb.column('docid')).index(_id)\n",
    "    probabilities[_index] = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add this probability as a new column to our table of canonic texts\n",
    "\n",
    "canon_tb['P(reviewed)'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "canon_tb.scatter('firstpub','P(reviewed)', fit_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  Q. Two of the prestigious texts are assigned less than 50% probability\n",
    "##     that they were reviewed. How do we make sense of that?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

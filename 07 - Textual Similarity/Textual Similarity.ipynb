{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to reproduce several findings from Andrew Piper's article \"Novel Devotions: Conversional Reading, Computational Modeling, and the Modern Novel\" (<i>New Literary History</i> 46.1 (2015), 63-98). See especially Fig 2 (p 72), Fig 4 (p 75), and Table 1 (p 79).\n",
    "\n",
    "Piper has made his research corpus of novels available here: http://txtlab.org/?p=601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "<li>Preparation</li>\n",
    "<li>Term Frequency Revisited</li>\n",
    "<li>Document-Term Matrix</li>\n",
    "<li>Normalization</li>\n",
    "<li>Streamlining</li>\n",
    "\n",
    "## Textual Similarity\n",
    "<li>Vector Space Model of Language</li>\n",
    "<li>Visualizing Texts in Vector Space</li>\n",
    "<li>Brief Aside: K-Means Clustering</li>\n",
    "<li>The Conversional Novel</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from datascience import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the Confessions from file, split into Books\n",
    "\n",
    "with open('Augustine - Confessions.txt') as file_in:\n",
    "    confession = file_in.read()\n",
    "confession_list = confession.split('\\n'*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each list entry is a string containing a Book of the Confessions\n",
    "\n",
    "confession_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Thirteen books in the Confessions, so hopefully that's the length of the list!\n",
    "\n",
    "len(confession_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of tokens from each text\n",
    "first_book = confession_list[1]\n",
    "first_token_list = first_book.lower().split()\n",
    "first_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Then use Counter to return a dictionary of tokens and their frequencies\n",
    "from collections import Counter\n",
    "word_freq = Counter(first_token_list)\n",
    "word_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Edit the script to return the ten most common words from the second book\n",
    "#      of the Confessions. How similar are they to those of the first book?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we plan to compare word frequencies across texts, then there is an easy\n",
    "# function that streamlines the process.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "dtm = cv.fit_transform(confession_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produces a 'sparse matrix.' Notice the dimensions.\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make this human-readable to build our intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# De-sparsify\n",
    "desparse = dtm.toarray()\n",
    "\n",
    "# Create labels for columns\n",
    "word_list = cv.get_feature_names()\n",
    "\n",
    "# Create a new Table\n",
    "dtm_tb = Table(word_list).with_rows(desparse)\n",
    "\n",
    "dtm_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can call up frequencies for a given word easily, since they are the column names\n",
    "\n",
    "dtm_tb['read']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q. Check-in: What do the values in the word columns represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the total number of words in the whole text\n",
    "sum(desparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to make apples-to-apples comparisons across Books, we can normalize our values\n",
    "# by dividing each word count by the total number of words in its Book.\n",
    "\n",
    "row_sums = np.sum(desparse, axis=1)\n",
    "normed = desparse/row_sums[:,None]\n",
    "dtm_tb = Table(word_list).with_rows(normed)\n",
    "\n",
    "dtm_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_tb['read']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a variety of reasons we like to remove words like \"the\", \"of\", \"and\", etc.\n",
    "# These are refered to as 'stopwords.'\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since we are using an older translation of Augustine, we have to remove archaic forms\n",
    "# of these stopwords as well.\n",
    "\n",
    "ye_olde_stop_words = ['thou','thy','thee', 'thine', 'ye', 'hath','hast', 'wilt','aught',\\\n",
    "                      'art', 'dost','doth', 'shall', 'shalt','tis','canst','thyself',\\\n",
    "                     'didst', 'yea', 'wert']\n",
    "\n",
    "stop_words = list(ENGLISH_STOP_WORDS)+ye_olde_stop_words\n",
    "\n",
    "# Remove stopwords from column list\n",
    "dtm_tb = dtm_tb.drop(stop_words)\n",
    "\n",
    "# It is often more efficient to perform operations on arrays rather than tables\n",
    "dtm_array = dtm_tb.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q. In the script above, we normalized term frequencies before removing stopwords.\n",
    "##    However, it would have been just as easy to do those steps in the opposite order.\n",
    "##    Are there situations where this decision has more or less of an impact on the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In fact, we can simply instruct CountVectorizer not to include stopwords at all\n",
    "# and another function, TfidfTransformer, normalizes easily.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cv = CountVectorizer(stop_words = stop_words)\n",
    "dtm = cv.fit_transform(confession_list)\n",
    "tt = TfidfTransformer(norm='l1',use_idf=False)\n",
    "dtm_tf = tt.fit_transform(dtm)\n",
    "\n",
    "word_list = cv.get_feature_names()\n",
    "dtm_array = dtm_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: If you are processing a text that uses only contemporary English, it may be\n",
    "#       unnecessary to import the list of stopwords explicitly. Simply pass the value\n",
    "#       \"english\" into the \"stop_words\" argument in CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Table(word_list).with_rows(dtm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. How many unique words were included in our list? How many unique words\n",
    "#     are there in total in the book (including stop words)?\n",
    "\n",
    "# EX. What is the Type-Token Ratio of Augustine's Confessions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model of Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's treat each document as a point (or vector) in space\n",
    "\n",
    "dtm_array = dtm_tf.toarray()\n",
    "dtm_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Each vector has a number of coordinates equal to the number of\n",
    "# unique words in the corpus.\n",
    "\n",
    "dtm_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Algebra 2: Euclidean Distance\n",
    "\n",
    "a = (2,6)\n",
    "b = (5,10)\n",
    "\n",
    "euc_dist = sqrt( (a[0]-b[0])**2  +  (a[1]-b[1])**2 )\n",
    "euc_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It also works in three dimensions!\n",
    "\n",
    "a = (2,6,15)\n",
    "b = (5,10,3)\n",
    "\n",
    "euc_dist = sqrt( (a[0]-b[0])**2 +  (a[1]-b[1])**2 + (a[2]-b[2])**2 )\n",
    "euc_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "distance.euclidean(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-Calculus & Linear Algebra: Cosine Distance\n",
    "\n",
    "a = (2,6)\n",
    "b = (5,10)\n",
    "\n",
    "# Don't worry about the formula so much as the intuition behind it: angle between vectors\n",
    "cos_dist = 1 - sum( a[0]*b[0] + a[1]*b[1] ) / ( sqrt(sum( a[0]**2 + a[1]**2 )) * sqrt(sum( b[0]**2 + b[1]**2 )))\n",
    "cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance.cosine(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Try passing different values into both the euclidean and cosine\n",
    "#     distance functions. What is your intution about these different measurements?\n",
    "\n",
    "#     Remember that all values in the Term-Frequency Matrix are positive,\n",
    "#     between [0,1], and that most are very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Texts in Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Measure distances among multiple points\n",
    "\n",
    "a = (2,6)\n",
    "b = (5,10)\n",
    "c = (14,11)\n",
    "\n",
    "print(distance.euclidean(a,b))\n",
    "print(distance.euclidean(a,c))\n",
    "print(distance.euclidean(b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Represent points as rows of matrix\n",
    "\n",
    "point_matrix = np.array([a,b,c])\n",
    "point_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distances among all rows of matrix\n",
    "\n",
    "from sklearn.metrics import pairwise\n",
    "pairwise.pairwise_distances(point_matrix, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distances among texts in vector space\n",
    "\n",
    "dist_matrix = pairwise.pairwise_distances(dtm_tf, metric='euclidean')\n",
    "\n",
    "title_list = ['Book '+str(i+1) for i in range(len(confession_list))]\n",
    "Table(title_list).with_rows(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multi-Dimensional Scaling\n",
    "\n",
    "# Measures the relative distances among points in high dimensional space\n",
    "# and projects them into two-dimensional space. (hand-waving the MDS algorithm for now)\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components = 2, dissimilarity=\"precomputed\")\n",
    "embeddings = mds.fit_transform(dist_matrix)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(13):\n",
    "    ax.annotate(i+1, ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Try visualizing the textual similarities again using the Cosine distance.\n",
    "#     How does that change the result? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Aside: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tries to find natural groupings among points, once we tell it\n",
    "# how many groups to look for. (Also hand-waving K-Means algorithm)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit_predict(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Try passing 'embeddings' and 'dtm_tf' as arguments into kmeans.fit_predict()\n",
    "#     Why do the clusters vary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Conversional Novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Operationalizing conversion in the novel\n",
    "\n",
    "def text_splitter(text):\n",
    "    n = int(len(text)/20)\n",
    "    text_list = [text[i*n:(i+1)*n] for i in range(20)]\n",
    "    return(text_list)\n",
    "\n",
    "def text_distances(text_list):\n",
    "    \n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    from sklearn.metrics import pairwise\n",
    "    \n",
    "    ye_olde_stop_words = ['thou','thy','thee', 'thine', 'ye', 'hath','hast', 'wilt','aught',\\\n",
    "                          'art', 'dost','doth', 'shall', 'shalt','tis','canst','thyself',\\\n",
    "                         'didst', 'yea', 'wert']\n",
    "    stop_words = list(ENGLISH_STOP_WORDS)+ye_olde_stop_words\n",
    "    cv = CountVectorizer(stop_words = stop_words, min_df=0.6)\n",
    "    dtm = cv.fit_transform(text_list)\n",
    "    tt = TfidfTransformer(norm='l1',use_idf=False)\n",
    "    dtm_tf = tt.fit_transform(dtm)\n",
    "    dist_matrix = pairwise.pairwise_distances(dtm_tf, metric='euclidean')\n",
    "    return(dist_matrix)\n",
    "\n",
    "def in_half_dist(matrix):\n",
    "    n = len(matrix)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for i in range(int(n/2)-1):\n",
    "        for j in range(i+1, int(n/2)):\n",
    "            d1.append(matrix[i,j])\n",
    "    for i in range(int(n/2), n-1):\n",
    "        for j in range(i+1, n):\n",
    "            d2.append(matrix[i,j])\n",
    "    return(abs(sum(d1)-sum(d2))/len(d1))\n",
    "\n",
    "\n",
    "def cross_half_dist(matrix):\n",
    "    n = len(matrix)\n",
    "    d = []\n",
    "    for i in range(int(n/2)):\n",
    "        for j in range(int(n/2), n):\n",
    "            d.append(matrix[i,j])\n",
    "    return(sum(d)/len(d))\n",
    "\n",
    "def text_measures(text):\n",
    "    text_list = text_splitter(text)\n",
    "    dist_matrix = text_distances(text_list)\n",
    "    return(cross_half_dist(dist_matrix), in_half_dist(dist_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test measurement on the Confessions\n",
    "\n",
    "text_measures(confession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get corpus metadata\n",
    "\n",
    "metadata_tb = Table.read_table('2_txtlab_Novel450.csv')\n",
    "metadata_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use just a single language, since there are likely to be different\n",
    "# norms across languages.\n",
    "\n",
    "metadata_tb = metadata_tb.where('language', \"English\")\n",
    "metadata_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify function to read texts from hard-drive\n",
    "# Integrates with Tables' \".apply()\" method and available metadata\n",
    "\n",
    "corpus_path = '2_txtalb_Novel450/'\n",
    "\n",
    "def text_measures_alt(text_name):\n",
    "    with open(corpus_path+text_name, 'r') as file_in:\n",
    "        text = file_in.read()\n",
    "    text_list = text_splitter(text)\n",
    "    dist_matrix = text_distances(text_list)\n",
    "    return(cross_half_dist(dist_matrix), in_half_dist(dist_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measures = metadata_tb.apply(text_measures_alt, 'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate the values from 'measures' into two separate Table columns\n",
    "\n",
    "metadata_tb['Cross-Half'] = measures[:,0]\n",
    "metadata_tb['In-Half'] = measures[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the Z-score of each value -- its number of standard deviations from the mean\n",
    "\n",
    "def get_zscores(values):\n",
    "\n",
    "    import numpy as np\n",
    "    mn = np.mean(values)\n",
    "    st = np.std(values)\n",
    "    zs = []\n",
    "    \n",
    "    for x in values:\n",
    "        z = (x-mn)/st\n",
    "        zs.append(z)\n",
    "\n",
    "    return zs\n",
    "\n",
    "metadata_tb['Cross-Z-Score'] = get_zscores(measures[:,0])\n",
    "metadata_tb['In-Z-Score'] = get_zscores(measures[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize!\n",
    "metadata_tb.scatter('In-Half', 'Cross-Half')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Even bigger!\n",
    "figure(figsize=(10,10))\n",
    "xlim((0,0.1))\n",
    "scatter(measures[:,1], measures[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Rankings for novels' Cross-Half and In-Half values\n",
    "\n",
    "cross_sort = metadata_tb.sort('Cross-Half', descending=True)['id']\n",
    "in_sort = metadata_tb.sort('In-Half', descending=True)['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average the Rankings from the two lists\n",
    "\n",
    "ranks = [ ( cross_sort.tolist().index(_id) + in_sort.tolist().index(_id) )/2 for _id in metadata_tb['id']]\n",
    "metadata_tb['Ranking'] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most conversional novels\n",
    "\n",
    "columns = ['author', 'title', 'Cross-Half', 'Cross-Z-Score', 'In-Half', 'In-Z-Score', 'Ranking']\n",
    "metadata_tb.select(columns).sort('Ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q.  Piper includes only words that appeared in at least 60% of the book's sections.\n",
    "#     How might that shape his findings? What if he had used a 50% threshold?\n",
    "\n",
    "# EX. Try changing the 'min_df' argument to 0.5. How do the rankings change?\n",
    "#     Try eliminating the 'min_df' altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Visualize distances among the twenty sections of the top-ranked\n",
    "#     conversional novel in the corpus using the MDS technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. When we processed our texts most recently, we removed stopwords before normalizing.\n",
    "#     Switch the order of these tasks. Does it change our findings? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
